[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "üî¨ The Academic and Professional (The Head)\nüåü Current Role & Mission\nI am Sun Jiahong (Ryan), a Postdoctoral Researcher specializing in Bioinformatics and Computational Biology at The Hong Kong Polytechnic University (PolyU) and Center for Eye and Vision Research (CEVR). My work centers on developing new algorithms for calling genetic variants and studying cancer evolution at single-cell level. I am also interested in a very new research field: AI virtual cell. I aim to translate complex biological datasets into actionable insights, particularly focusing on virtual cell modeling and disease mechanism discovery.\nüß¨ Research Focus & Expertise\n\nOmics Data Analysis: Expertise in WGS, single-cell DNA-seq, bulk RNA-seq and single-cell RNA-seq (scRNA-seq) data processing.\nStatistical Modeling: Advanced knowledge of Bayesian statistics, machine learning (supervised/unsupervised), and dimensionality reduction techniques.\nComputational Tools: Fluent in R (Tidyverse, Bioconductor) and Python (scikit-learn, PyTorch), with experience in cloud computing environments (AWS/GCP).\nKey Interest: Currently focused on studying genetic variations in cancers to find common cancer evolutionary patterns and risking factors.\n\nüöÄ What You‚Äôll Find Here\nThe bioinformatics section of this blog serves as a digital notebook for reproducible research. I share comprehensive tutorials, deep dives into statistical methods, and critiques of new tools. You‚Äôll find code and analysis for challenging projects related to omics data and AI virtual cell models.\n‚òï The Personal and Curated (The Heart)\nüìö Learning & Literacy\n\nReading: I am an avid reader of classic literature, focusing on [Author or Genre]. My non-fiction interests revolve around cognitive science and the philosophy of language.\nEnglish Learning: As an ESL speaker/lifelong learner, I share tips, resources, and reflections on improving fluency and professional communication.\n\nüç≥ Food & Fuel\n\nApproach to Food: My culinary philosophy mirrors my research: precise, experimental, and driven by data (i.e., ingredients ratios!). I focus on recipes that are high-nutrient, efficient for a busy schedule, and often inspired by [Cuisine Type].\n\nü§ù Connect and Collaborate\n\nEmail: The best way to reach me is via email: Email\nGitHub: View my repositories and source code for the blog here: GitHub\nGoogle Scholar: For my formal publication list: Google Scholar\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/weekly/20251213.html",
    "href": "posts/weekly/20251213.html",
    "title": "20251213-Weekly summary",
    "section": "",
    "text": "This is a test weekly summary page.\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/daily/20251215.html",
    "href": "posts/daily/20251215.html",
    "title": "20251215-UKBiobank",
    "section": "",
    "text": "Python skills\n\nÂÆö‰πâ‰∏Ä‰∏™ÂáΩÊï∞Ôºö def some_function(variable1: type, variable2: type) -&gt; type of return value„ÄÇ‰∏æ‰∏™Ê†óÂ≠êÔºödef read_copy_number_profiles(filepath: str) -&gt; Dict[str, np.ndarray]:\nÂÆö‰πâ‰∏Ä‰∏™ÂèòÈáèÔºövariable1: Dict[str, list[int]] = {}„ÄÇ‰∏æ‰∏™Ê†óÂ≠êÔºöprofiles: Dict[str, list[int]] = {}\nConvert lists to Numpy array: np.array(list, dtype=int)\nargparseÁî®Ê≥ï 4.1 ÂàùÂßãÂåñÔºöparser = argparse.ArgumentParser(description=‚Äú‚Äú) 4.2 Ê∑ªÂä†ÂèÇÊï∞: parser.add_argument(‚Äùinput_file‚Äù, help=‚Äú‚Äú), parser.add_argument(‚Äùoutput_file‚Äù, help=‚Äú‚Äú) 4.3 ÂåÖË£ÖÂèÇÊï∞: args = parser.parse_args() 4.4 Ë∞ÉÁî®ÂèÇÊï∞: input_file = args.input_file\n\n# Perform statistical test in Python\nfrom scipy import stats\ncml_counts = [10671,26081,26593,96335,110570]\nhealthy_counts = [101835,431203,183384]\n# T-test\nt_stat1, p_val1 = stats.ttest_ind(cml_counts, healthy_counts, equal_var=False)\n# Mann-Whiteney U test\nt_stat, p_val = stats.mannwhitneyu(cml_counts, healthy_counts, equal_var=False)\n\n\nLinux skills\n# Use AWK to filter rows, usage of && and ||\ngrep -v 'NormB' PC1_clusterA_metadata.csv | awk -F',' 'NR == 1 { print; next } {cnv = $4; ploidy = $5} cnv &gt; 0.03693037 && (ploidy &lt; 1.98583 || ploidy &gt; 2.015871)' \n\n\nApply for UK Biobank data access\nDeciphering the Immunogenetic Landscape: Epistatic Interactions between HLA and KIR Loci and their Impact on Human Health Phenotypes in the UK Biobank\n\nScientific Rationale 1.1 Background: Natural Killer (NK) cells are pivotal in the innate immune response against viral infections and tumors. Their function is tightly regulated by the interaction between KIR receptors (encoded on chromosome 19) and their specific HLA class I ligands (encoded on chromosome 6). 1.2 The Gap: Both gene families are highly polymorphic. While HLA associations with disease are well-documented, the epistatic (combinatorial) effect of HLA-KIR pairs is less understood, primarily because most studies lack the sample size to detect these complex interactions. 1.3 UK Biobank Advantage: The UKB offers a unique opportunity to study these interactions at scale. The availability of Whole Exome Sequencing (WES) and Whole Genome Sequencing (WGS) allows for the direct calling of complex KIR copy numbers and alleles, which can then be correlated with the rich phenotypic data available.\nResearch Questions 2.1 What is the landscape of KIR allelic diversity and copy number variation (CNV) across the UK Biobank population, and how does it stratify by ancestry? 2.2 How do specific epistatic combinations of KIR receptors and their cognate HLA ligands (e.g., KIR2DL1 with HLA-C2) influence susceptibility to immune-mediated diseases, infections, and cancer? 2.3 Can we identify novel pleiotropic effects of HLA-KIR interactions across a broad spectrum of phenotypes (PheWAS)?\nSpecific Objectives Objective 1: Immunogenetic Profiling. To determine high-resolution HLA alleles and KIR genotypes (including copy number and allelic variants) for UK Biobank participants. Note: HLA is already imputed in UKB, but KIR often requires running specific calling tools (like T1K or KIRCLE) on the WES/WGS data. Objective 2: Epistatic Interaction Mapping. To classify individuals based on functional NK cell ‚Äúeducation‚Äù status (e.g., Receptor-Ligand match vs.¬†mismatch) and define interaction terms (e.g., KIR3DL1+HLA-Bw4). Objective 3: Phenome-Wide Association Study (PheWAS). To perform a PheWAS to systematically screen for associations between HLA-KIR combinations and diverse phenotypes, with a focus on: Autoimmunity: Rheumatoid arthritis, Psoriasis, IBD. Infection: Viral susceptibility (e.g., severe influenza, COVID-19 severity). Malignancy: Hematological cancers and solid tumors.\n\nThis research will clarify the ‚Äúmissing heritability‚Äù in immune-mediated diseases by looking beyond single-gene effects to gene-gene interactions. Understanding these mechanisms could lead to better risk stratification for patients and potential therapeutic targets involving NK cell modulation (e.g., checkpoint inhibitors).\nThis research is in the public interest because it aims to improve the understanding of the genetic mechanisms underlying human immune response, specifically the interactions between HLA and KIR gene families. While these genes are known to be critical for fighting infections and cancer, their combined diversity is complex and poorly understood at the population level. By characterizing these variations in the UK Biobank, this project seeks to: 1. Identify genetic markers that predict susceptibility to immune-mediated diseases and cancer. 2. Clarify the ‚Äòmissing heritability‚Äô in complex disease traits that single-gene studies have failed to capture. Ultimately, this work aims to provide fundamental knowledge that could lead to improved risk stratification and the development of targeted immunotherapies. We are committed to publishing our findings in peer-reviewed scientific journals and sharing any developed bioinformatics tools with the research community to maximize the utility of this resource.\n\n\nHLA & KIR\n4-field allele calls 1: represent broad antigen group 2: indicate a unique protein variant, may affect T-cell binding 3: denote a sepcific silent variant, do not affect protein 4: non-coding region variations, including introns, 5‚Äô/3‚Äô UTRs, promoter and enhancer\nover 30,000 HLA alleles in IMGT/HLA database\nHLA locus (chromosome 6p21.3), ~5Mb, ~140 genes, Class I and II are core loci, most genes have copy number of 2, DRB3/4/5 generally have CNVs\nKIR locus (chromosome 19q13.42), ~150kb, 15 functional genes + 2 pseudogenes\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/daily/20251208.html",
    "href": "posts/daily/20251208.html",
    "title": "20251208-How to build a website on Github",
    "section": "",
    "text": "About three years ago, I built a blog website on Github using jekyll, now I want to re-start, so I asked Gemini 3 to help me. I followed its instructions and successfully built it.\n\n1. Archive previous site\n\nGo to your existing repository on GitHub.\nClick Settings -&gt; General.\nRename the repository to something like blog-archive-jekyll.\nCreate a new repository named sunjh22.github.io.\n\n\n\n2. Create new repository\n\nCreate a new repository named sunjh22.github.io.\nInitialize it with a README.md.\nGo to Settings &gt; Pages and set the ‚ÄúSource‚Äù to Deploy from a branch -&gt; main -&gt; /docs. (We will configure Quarto to render to a /docs folder).\n\n\n\n3. Download and install Quarto\nwget https://github.com/quarto-dev/quarto-cli/releases/download/v1.8.26/quarto-1.8.26-linux-amd64.tar.gz\ntar -xzf quarto-1.8.26-linux-amd64.tar.gz\nln -s /home/sunjh/data/biosoft/quarto-1.8.26/bin/quarto ~/data/bin/\n\n\n4. Initialize the Project\ngit clone https://github.com/sunjh22/sunjh22.github.io.git\ncd sunjh22.github.io\nquarto create-project myblog --type website:blog\nmv myblog/* .\nmv myblog/.gitignore .\nrmdir myblog\n\n\n5. Configure for GitHub Pages\nEdit _quarto.yml, index.qmd and about.qmd. All the new posts should be under ‚Äòposts/‚Äô directory, make three sub-directories ‚Äòposts/bioinfo‚Äô, ‚Äòposts/life‚Äô and ‚Äòposts/daily‚Äô.\n\n\n6. Preview and Publish\n# Because the port restriction, preview function is not available now.\nquarto preview\n\n# Render the full site\nquarto render\n\n# Push to Github\ngit add .\ngit commit -m \"Initial commit\"\ngit push origin main\n\n\nBonus for git push\n\nCheck for existing SSH keys:\n\nls -al ~/.ssh\nIf you see id_ed25519.pub or id_rsa.pub, skip to step 3.\n\nGenerate a new SSH key:\n\nssh-keygen -t ed25519 -C \"your_email@example.com\"\n(Press Enter through all prompts to accept defaults).\n\nAdd the key to GitHub: Display your public key:\n\ncat ~/.ssh/id_ed25519.pub\n# Or cat ~/.ssh/id_rsa.pub\nCopy the output (starts with ssh-ed25519‚Ä¶). Go to GitHub Settings ‚Üí SSH and GPG keys ‚Üí New SSH key. Paste the key and save.\n\nSwitch your repository to use SSH: Currently, your repo is looking for the HTTPS link. You need to point it to the SSH link.\n\n# Change remote URL from HTTPS to SSH\ngit remote set-url origin git@github.com:sunjh22/sunjh22.github.io.git\n\n# Verify it changed\ngit remote -v\n# Should show: origin  git@github.com:sunjh22/sunjh22.github.io.git (fetch)\n\nPush:\n\ngit push origin main\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/bioinfo/single-cell-analysis.html",
    "href": "posts/bioinfo/single-cell-analysis.html",
    "title": "Single Cell Analysis with Seurat",
    "section": "",
    "text": "Here is how I normalize data:\nlibrary(Seurat) # Code actually runs and displays output! pbmc &lt;- NormalizeData(pbmc)\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Home",
      "Bioinformatics",
      "Bioinfo",
      "Single Cell Analysis with Seurat"
    ]
  },
  {
    "objectID": "posts/life/spicy-tofy.html",
    "href": "posts/life/spicy-tofy.html",
    "title": "My Weekend Spicy Tofu Recipe",
    "section": "",
    "text": "Tofu\nChili Oil",
    "crumbs": [
      "Home",
      "Life & Recipes",
      "Life",
      "My Weekend Spicy Tofu Recipe"
    ]
  },
  {
    "objectID": "posts/life/spicy-tofy.html#ingredients",
    "href": "posts/life/spicy-tofy.html#ingredients",
    "title": "My Weekend Spicy Tofu Recipe",
    "section": "",
    "text": "Tofu\nChili Oil",
    "crumbs": [
      "Home",
      "Life & Recipes",
      "Life",
      "My Weekend Spicy Tofu Recipe"
    ]
  },
  {
    "objectID": "posts/daily/20251209.html",
    "href": "posts/daily/20251209.html",
    "title": "20251209-ATAC-seq v.s ChIP-seq",
    "section": "",
    "text": "ATAC-seq is used to find genomewide open regions that any protein could bind, including transcription binding site, promoters and enhancers (non-specific). Use hyperactive Tn5 transposase on unfixed nuclei, typically requires 500-50,000 cells as input, it is simple and fast.\nChIP-seq is used to find the binding site of specific proteins (e.g transcription factors) or screen the specific histone modifications by specific antibodies (specific). Use specific antibody against the protein/modification of interest on crosslinked (fixed) chromatin, typically requires 1-10 million cells, it is more labor-intensive.\nAs seen in ?@fig-histone, there are six common histone modifications. \n?@fig-antibody shows the current best-practice panel (what most big consortia and top labs use in 2024‚Äì2025) for mapping histone modifications. \n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/daily/20251216.html",
    "href": "posts/daily/20251216.html",
    "title": "20251216-Efficient-MST",
    "section": "",
    "text": "Highly efficient implementation minimum spanning tree in Python\n\"\"\"\nConstructs a Minimum Spanning Tree (MST) from single-cell copy number profiles\nbased on the Euclidean distance.\n\nOptimized version using pandas, scipy.spatial, and scipy.sparse.\n\"\"\"\n\nimport argparse\nimport sys\nimport pandas as pd\nimport numpy as np\nfrom scipy.spatial.distance import pdist, squareform\nfrom scipy.sparse.csgraph import minimum_spanning_tree\n\ndef main():\n    # ÂõõÊ≠•ÔºöÊûÑÂª∫ÂèÇÊï∞ÔºåÊ∑ªÂä†ÂèÇÊï∞ÔºåÂåÖË£ÖÂèÇÊï∞ÂíåË∞ÉÁî®ÂèÇÊï∞\n    parser = argparse.ArgumentParser(description=\"Build an MST from scCNV profiles using Euclidean Distance (Optimized).\")\n    parser.add_argument(\"input_file\", help=\"Path to the input CNV profile TSV file (bins as columns, cells as rows).\")\n    parser.add_argument(\"output_file\", help=\"Path for the output MST file.\")\n    args = parser.parse_args()\n\n    print(f\"Reading copy number profiles from: {args.input_file}\")\n    \n    # OPTIMIZATION 1: Use pandas for fast I/O\n    # Reads the entire matrix into memory efficiently. ÈÄüÂ∫¶Êõ¥Âø´ÔºåÂÜÖÂ≠òÊ∂àËÄóÊõ¥Â§ß\n    # header=None assumes the file has no header row (based on your demo file).\n    # index_col=0 uses the first column (Cell IDs) as the index.\n    try:\n        df = pd.read_csv(args.input_file, sep='\\t', header=None, index_col=0)\n    except Exception as e:\n        print(f\"Error reading file: {e}\")\n        sys.exit(1)\n        \n    n_cells, n_bins = df.shape\n    print(f\"Successfully read {n_cells} cells and {n_bins} bins.\")\n\n    if n_cells &lt; 2:\n        print(\"Error: Need at least 2 cells to build a tree.\")\n        sys.exit(1)\n\n    # OPTIMIZATION 2: Vectorized distance calculation\n    print(\"Calculating pairwise Euclidean distances...\")\n    # pdist computes pairwise distances in C, which is orders of magnitude faster than Python loops.\n    # It returns a condensed distance matrix to save memory.\n    try:\n        dist_condensed = pdist(df.values, metric='euclidean')\n        dist_matrix = squareform(dist_condensed)\n    except MemoryError:\n        print(\"Error: Not enough memory to compute the full distance matrix.\")\n        sys.exit(1)\n    except Exception as e:\n        print(f\"Error calculating distances: {e}\")\n        sys.exit(1)\n\n    # OPTIMIZATION 3: Efficient MST algorithm\n    print(\"Constructing Minimum Spanning Tree...\")\n    # uses efficient implementation (Kruskal's/Prim's) from scipy\n    mst_csr = minimum_spanning_tree(dist_matrix)\n    \n    # Convert the sparse matrix to Coordinate (COO) format to extract edges easily\n    mst_coo = mst_csr.tocoo()\n    \n    print(f\"Writing MST to: {args.output_file}\")\n    cell_ids = df.index\n    \n    try:\n        with open(args.output_file, 'w') as f:\n            f.write(\"from\\tto\\tdistance\\n\")\n            # Iterate through the edges found by the MST algorithm\n            for u, v, w in zip(mst_coo.row, mst_coo.col, mst_coo.data):\n                # Map numeric indices back to original Cell IDs\n                f.write(f\"{cell_ids[u]}\\t{cell_ids[v]}\\t{w}\\n\")\n    except IOError as e:\n        print(f\"Error writing output file: {e}\")\n        sys.exit(1)\n\n    print(\"Euclidean Tree inference finished successfully!\")\n\nif __name__ == \"__main__\":\n    main()\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "To be a better man",
    "section": "",
    "text": "20251216-Efficient-MST\n\n\n\ndaily\n\nPython\n\nMST\n\n\n\n\n\n\n\n\n\nDec 16, 2025\n\n\nRyan Sun\n\n\n\n\n\n\n\n\n\n\n\n\n20251215-UKBiobank\n\n\n\ndaily\n\n\n\n\n\n\n\n\n\nDec 15, 2025\n\n\nRyan Sun\n\n\n\n\n\n\n\n\n\n\n\n\n20251213-Weekly summary\n\n\n\nweekly\n\n\n\n\n\n\n\n\n\nDec 13, 2025\n\n\nRyan Sun\n\n\n\n\n\n\n\n\n\n\n\n\n20251209-ATAC-seq v.s ChIP-seq\n\n\n\ndaily\n\nATAC-seq\n\n\n\n\n\n\n\n\n\nDec 9, 2025\n\n\nRyan Sun\n\n\n\n\n\n\n\n\n\n\n\n\n20251208-How to build a website on Github\n\n\n\ndaily\n\ntutorial\n\n\n\n\n\n\n\n\n\nDec 8, 2025\n\n\nRyan Sun\n\n\n\n\n\n\n\n\n\n\n\n\nMy Weekend Spicy Tofu Recipe\n\n\n\nFood\n\nRecipes\n\nSpicy\n\n\n\n\n\n\n\n\n\nMay 21, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nSingle Cell Analysis with Seurat\n\n\n\nBioinformatics\n\nR\n\nscRNA-seq\n\n\n\n\n\n\n\n\n\nMay 20, 2024\n\n\n\n\n\nNo matching items\n Back to top"
  }
]